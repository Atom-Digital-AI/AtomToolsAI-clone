You are a senior full stack engineer. Implement the following “brand autocontext crawl” flow end to end. Use pragmatic, production grade code with clear separation between crawler, API, and UI. Prefer Typescript. Store vectors and page metadata so a RAG system can later retrieve context.

Goal
Build a crawl to review pipeline for up to 250 unique site pages, with URL deduplication, user tagging, exclusions, and meta description capture, then process and upsert for RAG.

Flow
Start a crawl job that collects up to 250 internal pages, honoring existing include and exclude rules.
Do not enqueue duplicate content URLs that only differ by query parameters or fragments.
After the crawl finishes, show a Review screen listing the 250 URLs with metadata so users can:
  classify each URL (include product category page as a new classification option)
  add an optional description up to 200 chars
  mark “exclude from future crawls” which updates the global exclusion rules accordingly
Auto populate description with each page’s HTML meta description, truncated to 200 characters.
On Save, process all reviewed pages: chunk, embed, and upsert into the vector store with page metadata and classification.

Key requirements and acceptance criteria
Crawler
  respect robots.txt, sitemap.xml, and a per domain rate limit
  normalize URLs by stripping querystrings and fragments for deduplication
  prefer canonical link rel="canonical" if present
  avoid session and tracking params such as utm_*, gclid, fbclid, ref
  enforce a hard cap of 250 unique normalized pages
  persist raw URL, normalized URL, canonical URL if any, HTTP status, content type, meta description, title, fetch time, hash of content

Review UI
  table listing raw URL, normalized URL, title, meta description preview, classification select, description text area with 200 char counter, exclude toggle
  bulk actions: set a classification for selected rows, toggle exclude for selected
  validation: cannot exceed 200 chars, cannot leave classification empty
  search and filter by classification, exclude state, and text search on URL and description

Exclusion management
  when user marks “exclude from future crawls” generate and store an exclusion rule that will match similar URLs (example strategies: prefix path rule, wildcard on query key, or regex)
  show the derived rule to the user for confirmation before saving
  store rules with type (path_prefix|glob|regex), pattern, example matched URLs, and created_by

Processing for RAG
  HTML to text extraction with boilerplate removal
  chunk into ~500 to 1,000 token windows with overlap
  embed each chunk and upsert to vector store with metadata:
    brand_id, page_id, normalized_url, canonical_url, title, meta_description, classification, excluded=false, crawl_timestamp, content_hash, chunk_id, chunk_index
  store full plain text for fallback search

APIs (suggested)
  POST /api/crawls  body: { brand_id, start_url, max_pages=250 }  returns crawl_id
  GET  /api/crawls/:id  returns status, counts, and any errors
  GET  /api/crawls/:id/pages  query: { q, classification, excluded } returns paginated list for review
  PATCH /api/pages/:id  body: { classification, description, exclude } upserts exclusion rules if exclude=true
  POST /api/crawls/:id/process  triggers chunk, embed, upsert and returns job status

Data models (suggested)
  Crawl(id, brand_id, start_url, status, created_at, finished_at, total_pages)
  Page(id, crawl_id, brand_id, raw_url, normalized_url, canonical_url, title, meta_description, http_status, content_type, content_hash, html_size, text_size, fetched_at)
  PageReview(id, page_id, classification, description, exclude, reviewer_id, reviewed_at)
  ExclusionRule(id, brand_id, type, pattern, sample_url, created_by, created_at)
  Chunk(id, page_id, chunk_index, tokens, text, embedding_vector_ref, created_at)

Classification values
  homepage
  product category page
  product detail page
  blog article
  help or docs
  legal or policy
  contact or about
  navigation or index
  other

Tech guidance
  Use a robust crawler with concurrency and politeness. Add per host delay and a global max concurrency.
  URL normalization: lowercase host, remove default ports, drop query and fragment for dedupe, collapse multiple slashes, remove trailing slash except root, decode percent encoding when safe.
  Canonical preference: if rel="canonical" points to same site, use it as normalized_url for dedupe and storage.
  Meta extraction: title, meta[name="description"] content, fallback to first paragraph text if empty.
  Exclusion rule inference: propose path_prefix from the page path, and for querystring based duplication infer a rule that ignores specific query keys.
  Vector store: pick a provider available in this project, store metadata and references to page_id and brand_id.
  Jobs: long running work for crawl and process should be background jobs with persisted status, idempotent, and resumable.

Validation and errors
  enforce 200 char description limit server side
  enforce non empty classification on save
  prevent processing if there are unreviewed pages
  show per page errors and an aggregate error banner
  retry transient fetch and embed failures

Testing guidance
  unit tests for URL normalization and canonical resolution
  fixtures for pages with tracking params to confirm dedupe works
  tests for exclusion rule generation and matching
  end to end test: crawl a small sample site, review pages, save, process, verify vector entries

Deliverables
  code with clear modules for crawler, parser, api, ui, and vector upsert
  seed scripts and example config
  documentation with run commands, environment variables, and a brief architecture overview